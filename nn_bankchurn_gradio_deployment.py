# -*- coding: utf-8 -*-
"""NN BankChurn Gradio Deployment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RbH5a30XAJE8lqH4iwsIt16IegfBfPSV

<center><font size=6> Bank Churn Prediction </font></center>

## Problem Statement

### Context

Businesses like banks which provide service have to worry about problem of 'Customer Churn' i.e. customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer's decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities.

### Objective

You as a Data scientist with the  bank need to  build a neural network based classifier that can determine whether a customer will leave the bank  or not in the next 6 months.

### Data Dictionary

* CustomerId: Unique ID which is assigned to each customer

* Surname: Last name of the customer

* CreditScore: It defines the credit history of the customer.
  
* Geography: A customerâ€™s location
   
* Gender: It defines the Gender of the customer
   
* Age: Age of the customer
    
* Tenure: Number of years for which the customer has been with the bank

* NumOfProducts: refers to the number of products that a customer has purchased through the bank.

* Balance: Account balance

* HasCrCard: It is a categorical variable which decides whether the customer has credit card or not.

* EstimatedSalary: Estimated salary

* isActiveMember: Is is a categorical variable which decides whether the customer is active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions etc )

* Exited : whether or not the customer left the bank within six month. It can take two values
** 0=No ( Customer did not leave the bank )
** 1=Yes ( Customer left the bank )
"""



"""## Importing necessary libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import plotly.figure_factory as ff
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score
import time
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from keras import backend
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import SGD, Adam , RMSprop
from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
import random
import warnings
warnings.filterwarnings('ignore')

### ONLY FFOR APPLYING IN HUGGINGFACES
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # force TensorFlow to CPU

import tensorflow as tf
from tensorflow.keras.models import load_model

"""## Loading the dataset"""

df=pd.read_csv('/content/bank-service.csv')

"""## Data Overview"""

df.head()
df.tail()

df.info()

df.describe()

df.describe(include='object')

df.isnull().sum()

df.duplicated().sum()

for i in df.columns:
  print(i,df[i].unique())

for i in df.columns:
  print(i,df[i].value_counts(normalize=True),'\n\n',"-"*40)

"""###Credit score and Age Have a lot of variables ,converting them to bins may help to understand the data and predict better"""

num_bins=5
df['CreditScore_bins']=pd.cut(df['CreditScore'],bins=num_bins,labels=[1,2,3,4,5],include_lowest=True)
df['Age_bins']=pd.cut(df['Age'],bins=num_bins,labels=[1,2,3,4,5],include_lowest=True)
df.head()

"""###get the ranges of the bins"""

credit_bins = pd.cut(df['CreditScore'], bins=num_bins, include_lowest=True)
age_bins = pd.cut(df['Age'], bins=num_bins, include_lowest=True)

# Print bin edges for CreditScore
print("CreditScore Bins Edges:")
credit_bin_edges = credit_bins.cat.categories  # Accessing the categories for CreditScores

# Extract the bin edges from the bins attribute directly
edges_credit = credit_bins.cat.categories
for i in range(len(edges_credit)):
    lower = edges_credit[i].left
    upper = edges_credit[i].right
    print(f"Bin {i + 1}: {lower} to {upper}")

# Print bin edges for Age
print("\nAge Bins Edges:")
age_bin_edges = age_bins.cat.categories  # Accessing the categories for Age

# Extract the bin edges from the bins attribute directly
edges_age = age_bins.cat.categories
for i in range(len(edges_age)):
    lower = edges_age[i].left
    upper = edges_age[i].right
    print(f"Bin {i + 1}: {lower} to {upper}")

df['CreditScore_bins'].value_counts()

df.groupby('CreditScore_bins')['Exited'].value_counts()

df.columns

"""###Drop RowNumbers ,customerID and surename as they dont impact our analysis and predictions"""

df.drop((['RowNumber', 'CustomerId', 'Surname']),axis=1,inplace=True)

df.head()

"""## Exploratory Data Analysis

### Univariate Analysis
"""

plt.figure(figsize=(15, 20))
for i, feature in enumerate(df.columns):
    plt.subplot(5, 3,i+1)  # 5 rows, 3 columns
    sns.histplot(data=df, x=feature, hue='Exited', multiple='stack', stat='percent')
    plt.title(f'Percentage of {feature}')


plt.tight_layout()
plt.show()

"""* Percentage of CreditScore:
customers are normally distributed  alonge the score with increase customers with scores above 800

* Percentage of Geography:
France has the highest number of customers while spain and germany nearly equal,percentage of existing customers in germany are greater than others

* Percentage of Gender:
male customers are more than female and existing in females is more may be due to unstable financial status and job opportunities

* Percentage of Age:
Age is skewed to the right  as most customers are in  younger ages while the existing is mostly in the medium age above 40

* Percentage of Balance:more the 35% of customers has balance less than 50000 and 5% of them are existing

* Percentage of NumOfProducts:
95% of customers have maximum 2 products which doesnt reflect clear loyality to the bank

* 70% of customers have credit cards which impact the bank loyality

* Percentage of IsActiveMember: 50% of customers are active indicating less engagment and use for bank services

* Percentage of Exited:
20% customers are exiting showing signal dissatisfaction or competitive pressures in the market.



"""

df_num=df.select_dtypes(include='number')

# Create a subplot grid with
plt.figure(figsize=(15, 20))
for i, feature in enumerate(df_num.columns):
    plt.subplot(5, 3,i+1)  # 5 rows, 3 columns
    #histogram_boxplot(data=df, feature=feature, kde=True)
    sns.boxplot(data=df, y=feature, x='Exited')
    plt.title(f'BOX of {feature}')


plt.tight_layout()
plt.show()

"""* Most of exiting are above age of 40
* customers with balance above 50000 are exiting more
"""

df_cat=df.select_dtypes(include=['object','category'])
# Create a subplot grid with
plt.figure(figsize=(15, 20))
for i, feature in enumerate(df_cat.columns):
    plt.subplot(5, 3,i+1)  # 5 rows, 3 columns
    sns.countplot(data=df, x=feature, hue='Exited')
    plt.title(f'count of {feature}')


plt.tight_layout()
plt.show()

df_num = df.select_dtypes(include='number')
num_features = len(df_num.columns)

# Create a subplot grid with 3 columns
for i, feature in enumerate(df_num.columns):
    fig = make_subplots(rows=1, cols=3, subplot_titles=(
        f'Histogram of {feature}',
        f'Boxplot of {feature}',
        f'Percentage of {feature}'
    ))

    # Add histogram to the first subplot
    hist = px.histogram(df, x=feature)
    for trace in hist.data:
        fig.add_trace(trace, row=1, col=1)

    # Add boxplot to the second subplot
    box = px.box(df, x=feature)
    for trace in box.data:
        fig.add_trace(trace, row=1, col=2)

    # Add percentage histogram to the third subplot
    percent_hist = px.histogram(df, x=feature, histnorm='percent')
    for trace in percent_hist.data:
        fig.add_trace(trace, row=1, col=3)


    fig.update_layout(height=400,width=1100)
    # Show the combined figure
    fig.show()

"""* 35% of customers are with Balance 2500
* 2.5% of the customers have 850 score
* Age is right skewed with alot of outliers values

### Bivariate Analysis
"""

plt.figure(figsize=(15, 5))
sns.heatmap(df_num.corr(),annot=True)

sns.pairplot(df,hue='Exited')

"""## Data Preprocessing

### Dummy Variable Creation
"""

df.info()
from sklearn.preprocessing import LabelEncoder
label_encoders = {}
data_cat = ['Geography', 'Gender', 'CreditScore_bins', 'Age_bins', 'CreditScore']
for col in data_cat:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

df.head()

"""### Train-validation-test Split"""



"""### Data Normalization"""

df.drop(['CreditScore_bins','Age_bins'],axis=1,inplace=True)
##df.drop(['CreditScore','Age'],axis=1,inplace=True)
x=df.drop('Exited',axis=1)
y=df['Exited']
x_temp,x_test,y_temp,y_test=train_test_split(x,y,test_size=0.2,random_state=1)
x_train,x_val,y_train,y_val=train_test_split(x_temp,y_temp,test_size=0.25,random_state=1)

sc=StandardScaler()


x_train[['Balance', 'EstimatedSalary']] = sc.fit_transform(x_train[['Balance', 'EstimatedSalary']])

x_val[['Balance', 'EstimatedSalary']] = sc.transform(x_val[['Balance', 'EstimatedSalary']])

x_test[['Balance', 'EstimatedSalary']] = sc.transform(x_test[['Balance', 'EstimatedSalary']])

#Printing the shapes.
print(x_train.shape,y_train.shape)
print(x_val.shape,y_val.shape)
print(x_test.shape,y_test.shape)

y_train.value_counts()

"""## Model Building

### Model Evaluation Criterion

Write down the logic for choosing the metric that would be the best metric for this business scenario.

-

### We will be using Recall as a metric for our model performance because here Bank could face 2 types of losses
1. identify a loyal customer as someone likely to churn, it could lead to unnecessary retention efforts
2. failing to identify a customer who will churn could result in lost revenue.

### Which Loss is greater?
* failing to identify a customer who will churn

### How to reduce this loss i.e need to reduce False Negatives?
* Bank wants recall to be maximized i.e. we need to reduce the number of false negatives.

In Machine learing classifiers the output is 1 or zero while in deep learning the output is based on sigmoide equation so we will need to defion a threshold ,above it is one and below it is zero
"""

def model_perf (model_name,model,predictors,target):
  y_pred=model.predict(predictors)
  y_pred=(y_pred>0.5)
  f1=f1_score(target,y_pred)
  acc=accuracy_score(target,y_pred)
  recall=recall_score(target,y_pred)
  precision=precision_score(target,y_pred)
  result={'Model':model_name,'f1_score':f1,'accuracy':acc,'recall':recall,'precision':precision}
  model_performance=pd.DataFrame([result])
  return model_performance

"""### Neural Network with SGD Optimizer"""

epochs = 80
batch_size = 32

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_sgd=Sequential()
model_sgd.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_sgd.add(Dense(8,activation='relu'))
model_sgd.add(Dense(2,activation='relu'))
model_sgd.add(Dense(1,activation='sigmoid'))
optimizer=SGD()
model_sgd.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
model_sgd.summary()

start=time.time()
history=model_sgd.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)

hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_sgd_perf_train=model_perf('model_sgd_train',model_sgd,x_train,y_train)
model_sgd_perf_val=model_perf('model_sgd_val',model_sgd,x_val,y_val)
print(model_sgd_perf_train)
print(model_sgd_perf_val)

"""## Model Performance Improvement

### Neural Network with Adam Optimizer
"""

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_adam=Sequential()
model_adam.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_adam.add(Dense(8,activation='relu'))
model_adam.add(Dense(2,activation='relu'))
model_adam.add(Dense(1,activation='sigmoid'))
optimizer=Adam()
model_adam.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])

print(model_adam.summary())
start=time.time()
history=model_adam.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)

hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_adam_perf_train=model_perf('model_adam_train',model_adam,x_train,y_train)
model_adam_perf_val=model_perf('model_adam_val',model_adam,x_val,y_val)
print(model_adam_perf_train)
print(model_adam_perf_val)

"""### Neural Network with Adam Optimizer and Dropout"""

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_adam=Sequential()
model_adam.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_adam.add(Dropout(0.4))
model_adam.add(Dense(8,activation='relu'))
model_adam.add(Dropout(0.4))
model_adam.add(Dense(2,activation='relu'))
model_adam.add(Dense(1,activation='sigmoid'))
optimizer=Adam()
model_adam.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_adam.summary())
start=time.time()
history=model_adam.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)

hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_adam_drop_train=model_perf('model_adam_drop_train',model_adam,x_train,y_train)
model_adam_drop_val=model_perf('model_adam_drop_val',model_adam,x_val,y_val)
print(model_adam_drop_train)
print(model_adam_drop_val)

"""### Neural Network with Balanced Data (by applying SMOTE) and SGD Optimizer"""

sampling=SMOTE()
x_train,y_train=sampling.fit_resample(x_train,y_train)

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_sgd=Sequential()
model_sgd.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_sgd.add(Dense(8,activation='relu'))
model_sgd.add(Dense(2,activation='relu'))
model_sgd.add(Dense(1,activation='sigmoid'))
optimizer=SGD()
model_sgd.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_sgd.summary())
start=time.time()
history=model_sgd.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)
hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_sgd_smote_train=model_perf('model_sgd_train_smote',model_sgd,x_train,y_train)
model_sgd_smote_val=model_perf('model_sgd_val_smote',model_sgd,x_val,y_val)
print(model_sgd_smote_train)
print(model_sgd_smote_val)

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_sgd=Sequential()
model_sgd.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_sgd.add(Dense(8,activation='relu'))
model_sgd.add(Dense(2,activation='relu'))
model_sgd.add(Dense(1,activation='sigmoid'))
optimizer=SGD(learning_rate=0.0001,momentum=0.9)
model_sgd.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_sgd.summary())
start=time.time()
history=model_sgd.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)
hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_sgd_mom_smote_train=model_perf('model_sgd_train_mom_smote',model_sgd,x_train,y_train)
model_sgd_mom_smote_val=model_perf('model_sgd_val_mom_smote',model_sgd,x_val,y_val)
print(model_sgd_mom_smote_train)
print(model_sgd_mom_smote_val)

"""### Neural Network with Balanced Data (by applying SMOTE) and Adam Optimizer"""

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_adam=Sequential()
model_adam.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_adam.add(Dense(8,activation='relu'))
model_adam.add(Dense(2,activation='relu'))
model_adam.add(Dense(1,activation='sigmoid'))
optimizer=Adam()
model_adam.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_adam.summary())
start=time.time()
history=model_adam.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)
hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_adam_smote_train=model_perf('model_adam_train_smote',model_adam,x_train,y_train)
model_adam_smote_val=model_perf('model_adam_val_smote',model_adam,x_val,y_val)
print(model_adam_smote_train)
print(model_adam_smote_val)

"""### Neural Network with Balanced Data (by applying SMOTE), Adam Optimizer, and Dropout"""

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_adam=Sequential()
model_adam.add(Dense(16,activation='relu',input_dim=x_train.shape[1]))
model_adam.add(Dropout(0.4))
model_adam.add(BatchNormalization())
model_adam.add(Dense(8,activation='relu'))
model_adam.add(Dropout(0.4))
model_adam.add(BatchNormalization())
model_adam.add(Dense(2,activation='relu'))
model_adam.add(Dense(1,activation='sigmoid'))
optimizer=Adam(learning_rate=0.0001)

model_adam.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_adam.summary())
start=time.time()
history=model_adam.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val))
end=time.time()
print(end-start)
hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_adam_smote_drop_train=model_perf('model_adam_train_drop_smote',model_adam,x_train,y_train)
model_adam_smote_drop_val=model_perf('model_adam_val_drop_smote',model_adam,x_val,y_val)
print(model_adam_smote_drop_train)
print(model_adam_smote_drop_val)

tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model_adam=Sequential()
model_adam.add(Dense(16,activation='relu',input_dim=x_train.shape[1],kernel_initializer='he_normal'))
model_adam.add(Dropout(0.4))
model_adam.add(BatchNormalization())
model_adam.add(Dense(8,activation='relu',kernel_initializer='he_normal'))
model_adam.add(Dropout(0.4))
model_adam.add(BatchNormalization())
model_adam.add(Dense(2,activation='relu',kernel_initializer='he_normal'))
model_adam.add(Dense(1,activation='sigmoid',kernel_initializer='he_normal'))
optimizer=Adam(learning_rate=0.0001)
es_cb=callbacks.EarlyStopping(monitor='val_recall',patience=5,min_delta=0.001,mode=max)
model_adam.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['recall'])
print(model_adam.summary())
start=time.time()
history=model_adam.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val),callbacks=[es_cb])
end=time.time()
print(end-start)
best_model_adam = model_adam
hist=pd.DataFrame(history.history)
hist['epoch']=history.epoch
plt.plot(hist['epoch'],hist['recall'],label='train')
plt.plot(hist['epoch'],hist['val_recall'],label='val')
plt.legend()
plt.show()
plt.plot(hist['epoch'],hist['loss'],label='train')
plt.plot(hist['epoch'],hist['val_loss'],label='val')
plt.legend()
plt.show()

model_adam_smote_drop_init_train=model_perf('model_adam_train_drop_init_smote',model_adam,x_train,y_train)
model_adam_smote_drop_init_val=model_perf('model_adam_val_drop_init_smote',model_adam,x_val,y_val)
print(model_adam_smote_drop_init_train)
print(model_adam_smote_drop_init_val)

"""# **##Save and check**"""

import os
os.makedirs("frontend_files",exist_ok=True)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
import joblib
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import os

# Load dataset
df = pd.read_csv('bank-service.csv')

# Drop irrelevant columns
df = df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], errors='ignore')

# Encode categorical variables
label_encoders = {}
categorical_cols = ['Geography', 'Gender']
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Split into features and target
X = df.drop('Exited', axis=1)
y = df['Exited']

# Train/val/test split
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1)

# Feature scaling
scaler = StandardScaler()
X_train[['Balance', 'EstimatedSalary']] = scaler.fit_transform(X_train[['Balance', 'EstimatedSalary']])
X_val[['Balance', 'EstimatedSalary']] = scaler.transform(X_val[['Balance', 'EstimatedSalary']])
X_test[['Balance', 'EstimatedSalary']] = scaler.transform(X_test[['Balance', 'EstimatedSalary']])

# SMOTE balancing
smote = SMOTE(random_state=1)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Model definition
tf.keras.backend.clear_session()
np.random.seed(2)
random.seed(2)
tf.random.set_seed(2)
model=Sequential()
model.add(Dense(16,activation='relu',input_dim=X_train.shape[1],kernel_initializer='he_normal'))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(8,activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(2,activation='relu',kernel_initializer='he_normal'))
model.add(Dense(1,activation='sigmoid',kernel_initializer='he_normal'))
optimizer=Adam(learning_rate=0.0001)
# Fix metric names lowercase for consistency:
es_cb = callbacks.EarlyStopping(monitor='val_recall', patience=5, min_delta=0.001, mode='max', restore_best_weights=True)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['recall'])

print(model.summary())

# Model training
history = model.fit(
    X_train, y_train,
    epochs=80,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[es_cb],
    verbose=1
)

# Save artifacts for deployment
os.makedirs("model_files", exist_ok=True)
model.save("model_files/bank_churn_best_model.h5")
joblib.dump(label_encoders, "model_files/label_encoders.joblib")
joblib.dump(scaler, "model_files/scaler.joblib")
joblib.dump(smote, "model_files/smote.joblib")
joblib.dump(list(X_train.columns), "model_files/model_columns.joblib")

print("âœ… All artifacts saved for Gradio backend deployment!")

model_para_train=model_perf('model_train',model,X_train,y_train)
model_para_val=model_perf('model_val',model,X_val,y_val)
model_para_test=model_perf('model_test',model,X_test,y_test)
print(model_para_train)
print(model_para_val)
print(model_para_test)

"""## **#Deployment**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile model_files/app.py
# import gradio as gr
# import pandas as pd
# import numpy as np
# import joblib
# import tensorflow as tf
# from tensorflow.keras.models import load_model
# 
# # Load saved artifacts
# tf.random.set_seed(2)
# np.random.seed(2)
# 
# model = load_model("bank_churn_best_model.h5", compile=False)
# label_encoders = joblib.load("label_encoders.joblib")
# scaler = joblib.load("scaler.joblib")
# model_columns = joblib.load("model_columns.joblib")
# 
# # Input feature names per training
# input_features = [
#     'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',
#     'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'
# ]
# 
# # Prepare options for categorical variables from encoders
# geography_options = list(label_encoders['Geography'].classes_)
# gender_options = list(label_encoders['Gender'].classes_)
# 
# def preprocess_input(data_dict):
#     # Create DataFrame from dictionary
#     df = pd.DataFrame([data_dict])
# 
#     # Encode categorical features using stored LabelEncoders
#     for col in ['Geography', 'Gender']:
#         if col in df.columns:
#             le = label_encoders[col]
#             df[col] = le.transform(df[col])
# 
#     # Scale numeric columns
#     df[['Balance', 'EstimatedSalary']] = scaler.transform(df[['Balance', 'EstimatedSalary']])
# 
#     # Reindex to model columns with 0 fill for any missing columns
#     df = df.reindex(columns=model_columns, fill_value=0)
# 
#     return df
# 
# def predict_churn(
#     CreditScore, Geography, Gender, Age, Tenure,
#     Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary
# ):
#     input_dict = {
#         'CreditScore': CreditScore,
#         'Geography': Geography,
#         'Gender': Gender,
#         'Age': Age,
#         'Tenure': Tenure,
#         'Balance': Balance,
#         'NumOfProducts': NumOfProducts,
#         'HasCrCard': HasCrCard,
#         'IsActiveMember': IsActiveMember,
#         'EstimatedSalary': EstimatedSalary
#     }
# 
#     processed_df = preprocess_input(input_dict)
#     pred_prob = model.predict(processed_df)[0][0]
#     prediction = int(pred_prob > 0.5)
#     status = "likely to churn" if prediction == 1 else "not likely to churn"
#     conf_score = f"Churn probability: {pred_prob:.3f}"
#     result_text = f"Customer is {status}."
#     return result_text, conf_score
# 
# # Create Gradio interface
# interface = gr.Interface(
#     fn=predict_churn,
#     inputs=[
#         gr.Number(label="Credit Score", value=600, precision=0),
#         gr.Dropdown(choices=geography_options, label="Geography"),
#         gr.Dropdown(choices=gender_options, label="Gender"),
#         gr.Number(label="Age", value=30, precision=0),
#         gr.Number(label="Tenure (years)", value=5, precision=0),
#         gr.Number(label="Balance", value=50000.0, precision=2),
#         gr.Number(label="Number of Products", value=1, precision=0),
#         gr.Dropdown(choices=[0, 1], label="Has Credit Card"),
#         gr.Dropdown(choices=[0, 1], label="Is Active Member"),
#         gr.Number(label="Estimated Salary", value=100000.0, precision=2),
#     ],
#     outputs=[
#         gr.Textbox(label="Churn Prediction"),
#         gr.Textbox(label="Confidence Score")
#     ],
#     title="Bank Customer Churn Prediction",
#     description="Enter customer data to predict their probability of leaving the bank in the next 6 months."
# )
# 
# if __name__ == "__main__":
#     interface.launch(server_name="0.0.0.0", server_port=7860)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile model_files/requirements.txt
# 
# gradio
# requests
# imbalanced-learn
# tensorflow
# joblib
# numpy
# pandas
# scikit-learn
# imblearn
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile model_files/Dockerfile
# FROM python:3.9-slim
# 
# WORKDIR /app
# 
# # (Optional) Install system dependencies required for plotting or Keras
# RUN apt-get update && apt-get install -y \
#     libglib2.0-0 libsm6 libxrender1 libxext6 && \
#     rm -rf /var/lib/apt/lists/*
# 
# # Copy requirements and install
# COPY requirements.txt ./
# RUN pip install --no-cache-dir --upgrade pip && \
#     pip install --no-cache-dir -r requirements.txt
# 
# # Copy all necessary files from model_files to the container workdir
# COPY bank_churn_best_model.h5 ./
# COPY label_encoders.joblib ./
# COPY model_columns.joblib ./
# COPY scaler.joblib ./
# COPY smote.joblib ./
# COPY app.py ./
# 
# EXPOSE 7860
# 
# CMD ["python", "app.py"]
#

"""**#HuggingFaces space for backend**"""

from google.colab import userdata
from huggingface_hub import login, HfApi

access_key = userdata.get('HF_TOKEN')  # Retrieve your secret by name
repo_id = "Afifi00/Gradio_BankChurn_Prediction"  # Your Hugging Face space id

# Login to Hugging Face platform with the access token
login(token=access_key)

# Initialize the API
api = HfApi()

# Upload Streamlit app files stored in the folder called deployment_files
api.upload_folder(
    folder_path="/content/model_files",  # Local folder path
    repo_id=repo_id,  # Hugging face space id
    repo_type="space",  # Hugging face repo type "space"
)

import shutil
shutil.make_archive('backend_files', 'zip', 'model_files')
from google.colab import files
files.download('backend_files.zip')

"""## Model Performance Comparison and Final Model Selection

###Summary of Results
The models show varying performance across different configurations, evaluated using F1 Score, Accuracy, Recall, and Precision.
The Adam optimizer generally outperforms the SGD variants, particularly when using SMOTE to handle class imbalance.
The recall scores for the different models indicate how well the models are capturing positive cases.

Performance of Adam Optimizer:
Without Dropout:
Recall (Train): 0.142502 (model_adam_train)
Recall (Val): 0.115663 (model_adam_val)
With SMOTE:
Recall (Train): 0.857083 (model_adam_train_smote)
Recall (Val): 0.701205 (model_adam_val_smote)
The Adam optimizer significantly improves recall when using SMOTE, indicating it effectively learns from the minority class in the dataset.

Impact of SMOTE:
The use of SMOTE drastically enhances recall scores:
Recall (Train): Increased from 0.142502 to 0.857083.
Recall (Val): Increased from 0.115663 to 0.701205.
This improvement suggests that SMOTE is successful in balancing the dataset, allowing the model to better recognize positive instances.

Dropout's Effect:
With Dropout:
Recall scores are lower:
Train Recall: 0.011599 (model_adam_drop_train)
Val Recall: 0.012048 (model_adam_drop_val)
The significant drop in recall indicates that dropout may have led to underfitting in this particular setup, preventing the model from effectively learning important patterns.

Initialization Strategies:
With Dropout and Initialization:
Train Recall: 0.991863 (model_adam_train_drop_init_smote)
Val Recall: 0.992771 (model_adam_val_drop_init_smote)
These high recall scores suggest that careful initialization, combined with SMOTE, allows the model to maintain performance while also using dropout for regularization.

###Model optimizer=Adam(learning_rate=0.0001) with Dropout,initialization,batch normalization and earlystop will be used to predict the test set and to be assigned for the project
"""

final_result=pd.concat([model_sgd_perf_train,model_sgd_perf_val,model_adam_perf_train,model_adam_perf_val,model_adam_drop_train,model_adam_drop_val,model_sgd_smote_train,model_sgd_smote_val,model_sgd_mom_smote_train,model_sgd_mom_smote_val,model_adam_smote_train,model_adam_smote_val,model_adam_smote_drop_train,model_adam_smote_drop_val,model_adam_smote_drop_init_train,model_adam_smote_drop_init_val],axis=0)
final_result

final_result.T

model_sgd_test=model_perf('model_sgd_test',model_sgd,x_test,y_test)
model_adam_test=model_perf('model_adam_test',model_adam,x_test,y_test)
model_adam_drop_test=model_perf('model_adam_drop_test',model_adam,x_test,y_test)
model_sgd_smote_test=model_perf('model_sgd_smote_test',model_sgd,x_test,y_test)
model_adam_smote_test=model_perf('model_adam_smote_test',model_adam,x_test,y_test)
model_adam_smote_drop_test=model_perf('model_adam_smote_drop_test',model_adam,x_test,y_test)
model_adam_smote_drop_init_test=model_perf('model_adam_smote_drop_init_test',model_adam,x_test,y_test)
final_test_result=pd.concat([model_sgd_test,model_adam_test,model_adam_drop_test,model_sgd_smote_test,model_adam_smote_test,model_adam_smote_drop_test,model_adam_smote_drop_init_test],axis=0)
final_test_result

"""### Neural Network with Balanced Data (by applying SMOTE),Run all optimizer types with dropout,Batch normalization, weight initialization and earlystop"""

optimizer=[SGD(),SGD(learning_rate=0.0001,momentum=0.9),Adam(),Adam(learning_rate=0.0001),RMSprop(),RMSprop(learning_rate=0.0001)]
epochs=80
batch_size=32

result_train=pd.DataFrame()
result_val=pd.DataFrame()
result_test=pd.DataFrame()
for i in optimizer:
  tf.keras.backend.clear_session()
  np.random.seed(2)
  random.seed(2)
  tf.random.set_seed(2)
  New_model=Sequential()
  New_model.add(Dense(16,activation='relu',input_dim=x_train.shape[1],kernel_initializer='he_normal'))
  New_model.add(Dropout(0.4))
  New_model.add(BatchNormalization())
  New_model.add(Dense(8,activation='relu',kernel_initializer='he_normal'))
  New_model.add(Dropout(0.4))
  New_model.add(BatchNormalization())
  New_model.add(Dense(2,activation='relu',kernel_initializer='he_normal'))
  New_model.add(Dense(1,activation='sigmoid',kernel_initializer='he_normal'))
  es_cb=callbacks.EarlyStopping(monitor='val_recall',patience=5,min_delta=0.001,mode=max)
  New_model.compile(optimizer=i,loss='binary_crossentropy',metrics=['recall'])
  print(i)
  #print(New_model.summary())
  start=time.time()
  history=New_model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_val,y_val),callbacks=[es_cb],verbose=0)
  end=time.time()
  print(end-start)
  hist=pd.DataFrame(history.history)
  hist['epoch']=history.epoch
  plt.plot(hist['epoch'],hist['recall'],label='train')
  plt.plot(hist['epoch'],hist['val_recall'],label='val')
  plt.legend()
  #plt.show()
  plt.plot(hist['epoch'],hist['loss'],label='train')
  plt.plot(hist['epoch'],hist['val_loss'],label='val')
  plt.legend()
  #plt.show()
  New_model_train=model_perf(i.__class__.__name__ ,New_model,x_train,y_train)
  New_model_val=model_perf(i.__class__.__name__ ,New_model,x_val,y_val)
  New_model_test=model_perf(i.__class__.__name__ ,New_model,x_test,y_test)
  result_train=pd.concat([result_train,New_model_train],axis=0)
  result_val=pd.concat([result_val,New_model_val],axis=0)
  result_test=pd.concat([result_test,New_model_test],axis=0)

print(result_train,'\n',50*'-','\n',result_val,'\n',50*'-','\n',result_test)

"""## Actionable Insights and Business Recommendations

* Enhance Customer Engagement:

Increase Active Member Engagement: Since only 50% of customers are active, implementing targeted engagement strategies (e.g., personalized offers, reminders for account activities) could encourage more frequent use of bank services.

Loyalty Programs: Develop loyalty programs that reward customers for using multiple products or maintaining higher balances. This could increase customer retention.
* Focus on Customer Segmentation:

Target Older Customers: Given that most exits are from customers above the age of 40, consider creating targeted products or services that cater specifically to their needs, such as retirement planning or investment advice.

Gender-Specific Strategies:
Analyze the reasons behind female customers' instability in financial status. Tailored financial education programs or support could help improve their financial health and loyalty.

* Improve Customer Support:

Dedicated Support Channels: Establish dedicated support channels for at-risk customers, particularly those with low balances or fewer products. Proactive outreach can help address concerns before they lead to churn.
Feedback Mechanisms: Implement feedback mechanisms (e.g., surveys) to understand why customers are considering leaving. This can provide insights into specific pain points that need addressing.

* Optimize Product Offerings:

Product Diversification: Since 95% of customers have a maximum of two products, consider offering bundled products or incentives for customers to explore additional services (e.g., loans, insurance).
Credit Card Benefits: With 70% of customers having credit cards, enhancing the benefits associated with credit card usage (cashback, rewards) could encourage loyalty.

* Monitor Financial Health:

Balance Management: Since customers with balances above 50,000 are exiting more frequently, analyze this group to understand if they feel their needs are being met. Provide financial advisory services aimed at wealth management.

Educational Resources: Offer resources to help customers better manage their finances, particularly those with lower balances who may be at risk of exiting.

<font size=6 color='blue'>Power Ahead</font>
___
"""